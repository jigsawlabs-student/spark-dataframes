{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fuzzy-facial",
   "metadata": {},
   "source": [
    "# Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-granny",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-venue",
   "metadata": {},
   "source": [
    "Now normally, when working with Pyspark, we work on a higher level of abstraction, which is to work with a dataframe.  In this lesson, we'll get started working with a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-ability",
   "metadata": {},
   "source": [
    "### Introducing a Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-bottom",
   "metadata": {},
   "source": [
    "The first step to creating a dataframe, is to initialize a spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9907a740-c1b3-4c82-a5a8-f3e7df7f3c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to reinstall pyspark in colab\n",
    "\n",
    "!pip install fsspec --quiet\n",
    "!pip install s3fs --quiet\n",
    "!pip install pyspark --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "developing-preserve",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/14 12:36:10 WARN Utils: Your hostname, Jeffreys-MacBook-Air-2.local resolves to a loopback address: 127.0.0.1; using 10.117.97.200 instead (on interface en0)\n",
      "23/09/14 12:36:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/14 12:36:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-duplicate",
   "metadata": {},
   "source": [
    "Now, a spark session is very similar to a spark context, which we worked with in previous lessons.  A spark session, wraps around a spark context -- creating a new spark context if one doesn't currently exist. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-gender",
   "metadata": {},
   "source": [
    "> So if we want to get the spark context from our session we can do so with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "structural-lighter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.117.97.200:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL basic example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Python Spark SQL basic example>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-stream",
   "metadata": {},
   "source": [
    "So this spark session is really just a thin wrapper around our spark context, which is one way for us to connect to our spark cluster.  The spark session is the other way.\n",
    "\n",
    "Now let's use the spark session to create a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-declaration",
   "metadata": {},
   "source": [
    "### Creating a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-pakistan",
   "metadata": {},
   "source": [
    "To create our dataframe, we can start with a list of dictionaries in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "oriental-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = [{'index': 1,\n",
    "  'title': 'Shazam!',\n",
    "  'release_date': 1553299200,\n",
    "  'genre': 'Comedy'}, {'index': 2,\n",
    "  'title': 'Captain Marvel',\n",
    "  'release_date': 1551830400,\n",
    "  'genre': 'Action'},  {'index': 3,\n",
    "  'title': 'Escape Room',\n",
    "  'release_date': 1546473600,\n",
    "  'genre': 'Horror'}, {'index': 4,\n",
    "  'title': 'How to Train A Dragon',\n",
    "  'release_date': 1546473600,\n",
    "  'genre': 'Animation'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-bridge",
   "metadata": {},
   "source": [
    "> So here we have a list of movies displaying the `title`, `release_date` and `genre` of each movie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-stress",
   "metadata": {},
   "source": [
    "And then we can use the `createDataFrame` method on the spark session to create our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "becoming-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = spark.createDataFrame(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-uganda",
   "metadata": {},
   "source": [
    "And we can view that dataframe by running the `show` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "turned-hamilton",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------------+--------------------+\n",
      "|    genre|index|release_date|               title|\n",
      "+---------+-----+------------+--------------------+\n",
      "|   Comedy|    1|  1553299200|             Shazam!|\n",
      "|   Action|    2|  1551830400|      Captain Marvel|\n",
      "|   Horror|    3|  1546473600|         Escape Room|\n",
      "|Animation|    4|  1546473600|How to Train A Dr...|\n",
      "+---------+-----+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-mitchell",
   "metadata": {},
   "source": [
    "So we can see from the above, that our dataframe organizes our data in a table.  It has associated our records with various columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-fiber",
   "metadata": {},
   "source": [
    "We can also see the *schema on read* characteristic from spark.  That even without specifying a datatype, Spark was able to determine the datatype for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "other-system",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- genre: string (nullable = true)\n",
      " |-- index: long (nullable = true)\n",
      " |-- release_date: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-credit",
   "metadata": {},
   "source": [
    "### From DataFrame to RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-brake",
   "metadata": {},
   "source": [
    "Now a dataframe in Pyspark creates an RDD under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "composed-habitat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[10] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "unlikely-herald",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(genre='Comedy', release_date=1553299200, title='Shazam!'),\n",
       " Row(genre='Action', release_date=1551830400, title='Captain Marvel'),\n",
       " Row(genre='Horror', release_date=1546473600, title='Escape Room'),\n",
       " Row(genre='Animation', release_date=1546473600, title='How to Train A Dragon')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-ensemble",
   "metadata": {},
   "source": [
    "1. It's distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-belly",
   "metadata": {},
   "source": [
    "And that even though this looks like a unified dataset, it's really distributed across different nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "unlike-insured",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-launch",
   "metadata": {},
   "source": [
    "2. It's lazy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-consent",
   "metadata": {},
   "source": [
    "Because our dataset is built on RDDs, is also operates in lazy manner.  So for example, if we want to select all of the titles of an RDD, we'll use a `map` function to select the title from each row.  But because `map` is a transformation, it will not operate on our data, until we follow up with an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "featured-producer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[11] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.rdd.map(lambda movie: movie['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "concrete-friendship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Shazam!', 'Captain Marvel', 'Escape Room', 'How to Train A Dragon']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.rdd.map(lambda movie: movie['title']).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-istanbul",
   "metadata": {},
   "source": [
    "If we perform the equivalent operation with a dataframe, the operation is also treated as a transformation.  Let's see this.  Below, we'll select the `title` of each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "excited-durham",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[title: string]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.select('title')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-national",
   "metadata": {},
   "source": [
    "So again, spark will not search through each of the records until an action is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "monthly-saudi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               title|\n",
      "+--------------------+\n",
      "|             Shazam!|\n",
      "|      Captain Marvel|\n",
      "|         Escape Room|\n",
      "|How to Train A Dr...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.select('title').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-assault",
   "metadata": {},
   "source": [
    "> So we can see that `show` is similar to `collect`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-phone",
   "metadata": {},
   "source": [
    "Let's do this one more time, this time with two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "changed-bloom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|               title|    genre|\n",
      "+--------------------+---------+\n",
      "|             Shazam!|   Comedy|\n",
      "|      Captain Marvel|   Action|\n",
      "|         Escape Room|   Horror|\n",
      "|How to Train A Dr...|Animation|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.select(['title', 'genre']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-velvet",
   "metadata": {},
   "source": [
    "> So to select multiple columns, we pass through a list of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-baseball",
   "metadata": {},
   "source": [
    "4. Only Coarse Grained Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-shipping",
   "metadata": {},
   "source": [
    "Remember that with our RDDs, we only have coarse grained methods available to us -- those methods like `map` or `filter` that operate across a dataset.  This makes things a little tricky if we want to just select a single row.  For example, we may think that with our dataframe above, we want to select the entry at a specific index.  With our dataframe, the only way to do this is to use something akin to the filter method -- where we ask to *select* the rows that have an id of 1.  But we'll learn how to do that with our dataframe in a future lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-violence",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-checkout",
   "metadata": {},
   "source": [
    "In this lesson we learned how to create a DataFrame in Spark.  We do so, not through our Spark context but by creating a Spark session.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dying-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "lyric-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = spark.createDataFrame(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-grant",
   "metadata": {},
   "source": [
    "We then saw that our dataframe is really just a simpler interface for interacting with our resilient distributed dataset.  And because of this, it contains the same features of our RDD: it's distributed, it's lazy, and allows for only coarse grained transformations.\n",
    "\n",
    "This knowledge gave us some insight into how to interact with our dataframe.  So we saw that to select specific columns, we have to use the `select` method, which operates as a `transformation` and then use the `show` method as our action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "proprietary-money",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|               title|    genre|\n",
      "+--------------------+---------+\n",
      "|             Shazam!|   Comedy|\n",
      "|      Captain Marvel|   Action|\n",
      "|         Escape Room|   Horror|\n",
      "|How to Train A Dr...|Animation|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.select(['title', 'genre']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-province",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Pyspark operations](https://hendra-herviawan.github.io/)\n",
    "\n",
    "[Pyspark DataFrame Rows and Columns](https://hendra-herviawan.github.io/pyspark-dataframe-row-columns.html)\n",
    "\n",
    "[Creating a Dataframe](https://neapowers.com/pyspark/createdataframe-todf/)\n",
    "\n",
    "[Spark by Examples](https://sparkbyexamples.com/pyspark-tutorial/#pyspark-dataframe)\n",
    "\n",
    "[Data Partitioning Spark](https://kontext.tech/column/spark/296/data-partitioning-in-spark-pyspark-in-depth-walkthrough)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-trouble",
   "metadata": {},
   "source": [
    "[DataBricks RDD to Dataframe](https://databricks.com/glossary/what-is-rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-circuit",
   "metadata": {},
   "source": [
    "[DataFrame Programming Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
