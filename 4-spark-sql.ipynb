{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incorrect-pixel",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-patch",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7465a3db-2fe6-4da0-be2a-1b73dc66ea88",
   "metadata": {},
   "source": [
    "In this lesson, we'll demo how to query spark with Spark SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-circus",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4903882-7318-4990-a56b-bcc5d52dc097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to reinstall pyspark in colab\n",
    "\n",
    "!pip install fsspec --quiet\n",
    "!pip install s3fs --quiet\n",
    "!pip install pyspark --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "reflected-sponsorship",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "following-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"s3://jigsaw-labs-student/houston_claims.csv\"\n",
    "df = pd.read_csv(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c7de98-3fcd-41dc-a711-4f4ecdc04cf9",
   "metadata": {},
   "source": [
    "After loading our data from s3, we can create our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872ac144-5f8e-4c16-937a-a46262a21d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_df = spark.createDataFrame(df.astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a69eb6b-0098-4970-9009-0475f8650d2b",
   "metadata": {},
   "source": [
    "And then create a temporary view, which will allow us to query our claims as if it were a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "atlantic-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_df.createOrReplaceTempView(\"claims\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918af646-2e7c-4239-8546-c3a8cc3234ed",
   "metadata": {},
   "source": [
    "Let's look at the claims schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dirty-uniform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Unnamed: 0: string (nullable = true)\n",
      " |-- reportedCity: string (nullable = true)\n",
      " |-- dateOfLoss: string (nullable = true)\n",
      " |-- elevatedBuildingIndicator: string (nullable = true)\n",
      " |-- floodZone: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- lowestFloodElevation: string (nullable = true)\n",
      " |-- amountPaidOnBuildingClaim: string (nullable = true)\n",
      " |-- amountPaidOnContentsClaim: string (nullable = true)\n",
      " |-- yearofLoss: string (nullable = true)\n",
      " |-- reportedZipcode: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "claims_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44f8c6f-2feb-4a17-a35c-3a7c443e9709",
   "metadata": {},
   "source": [
    "And now we can simply query spark like sql.  Remember that to execute the job, we need to call an action like `show`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "animal-sunday",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------\n",
      " Unnamed: 0                | 2469                 \n",
      " reportedCity              | HOUSTON              \n",
      " dateOfLoss                | 1985-11-17T00:00:... \n",
      " elevatedBuildingIndicator | False                \n",
      " floodZone                 | C                    \n",
      " latitude                  | 29.8                 \n",
      " longitude                 | -95.5                \n",
      " lowestFloodElevation      | nan                  \n",
      " amountPaidOnBuildingClaim | 0.0                  \n",
      " amountPaidOnContentsClaim | 0.0                  \n",
      " yearofLoss                | 1985-01-01T00:00:... \n",
      " reportedZipcode           | 77024                \n",
      " id                        | 5e398d7074cbd479f... \n",
      "-RECORD 1-----------------------------------------\n",
      " Unnamed: 0                | 2468                 \n",
      " reportedCity              | HOUSTON              \n",
      " dateOfLoss                | 1985-09-29T00:00:... \n",
      " elevatedBuildingIndicator | False                \n",
      " floodZone                 | C                    \n",
      " latitude                  | 29.8                 \n",
      " longitude                 | -95.5                \n",
      " lowestFloodElevation      | nan                  \n",
      " amountPaidOnBuildingClaim | 0.0                  \n",
      " amountPaidOnContentsClaim | 0.0                  \n",
      " yearofLoss                | 1985-01-01T00:00:... \n",
      " reportedZipcode           | 77024                \n",
      " id                        | 5e398d7074cbd479f... \n",
      "-RECORD 2-----------------------------------------\n",
      " Unnamed: 0                | 4029                 \n",
      " reportedCity              | HOUSTON              \n",
      " dateOfLoss                | 1985-09-30T00:00:... \n",
      " elevatedBuildingIndicator | False                \n",
      " floodZone                 | C                    \n",
      " latitude                  | 29.8                 \n",
      " longitude                 | -95.6                \n",
      " lowestFloodElevation      | nan                  \n",
      " amountPaidOnBuildingClaim | 2138.5               \n",
      " amountPaidOnContentsClaim | 0.0                  \n",
      " yearofLoss                | 1985-01-01T00:00:... \n",
      " reportedZipcode           | 77079                \n",
      " id                        | 5e398d7374cbd479f... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM claims ORDER BY yearofLoss LIMIT 3;\").show(vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-hostel",
   "metadata": {},
   "source": [
    "> Now we can also use a WHERE clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "distinct-spending",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------\n",
      " _c0                       | 2                    \n",
      " reportedCity              | HOUSTON              \n",
      " dateOfLoss                | 2004-06-28 20:00:00  \n",
      " elevatedBuildingIndicator | false                \n",
      " floodZone                 | X                    \n",
      " latitude                  | 29.8                 \n",
      " longitude                 | -95.6                \n",
      " lowestFloodElevation      | null                 \n",
      " amountPaidOnBuildingClaim | 1420.89              \n",
      " amountPaidOnContentsClaim | 0.0                  \n",
      " yearofLoss                | 2003-12-31 19:00:00  \n",
      " reportedZipcode           | 77042                \n",
      " id                        | 5e398d6774cbd479f... \n",
      "-RECORD 1-----------------------------------------\n",
      " _c0                       | 3                    \n",
      " reportedCity              | HOUSTON              \n",
      " dateOfLoss                | 2009-04-27 20:00:00  \n",
      " elevatedBuildingIndicator | false                \n",
      " floodZone                 | X                    \n",
      " latitude                  | 29.8                 \n",
      " longitude                 | -95.6                \n",
      " lowestFloodElevation      | null                 \n",
      " amountPaidOnBuildingClaim | 2019.66              \n",
      " amountPaidOnContentsClaim | 0.0                  \n",
      " yearofLoss                | 2008-12-31 19:00:00  \n",
      " reportedZipcode           | 77042                \n",
      " id                        | 5e398d6774cbd479f... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM claims WHERE latitude = 29.8 LIMIT 2;\").show(vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08518564-d72a-42a0-b010-5745296d3dba",
   "metadata": {},
   "source": [
    "And if you want to see what is occurring underneath, you can use the `explain` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "authentic-sport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "CollectLimit 2\n",
      "+- *(1) Filter (isnotnull(latitude#155) AND (latitude#155 = 29.8))\n",
      "   +- FileScan csv [_c0#150,reportedCity#151,dateOfLoss#152,elevatedBuildingIndicator#153,floodZone#154,latitude#155,longitude#156,lowestFloodElevation#157,amountPaidOnBuildingClaim#158,amountPaidOnContentsClaim#159,yearofLoss#160,reportedZipcode#161,id#162] Batched: false, DataFilters: [isnotnull(latitude#155), (latitude#155 = 29.8)], Format: CSV, Location: InMemoryFileIndex[file:/Users/jeff/Library/Mobile Documents/com~apple~CloudDocs/Documents/jigsaw/..., PartitionFilters: [], PushedFilters: [IsNotNull(latitude), EqualTo(latitude,29.8)], ReadSchema: struct<_c0:int,reportedCity:string,dateOfLoss:timestamp,elevatedBuildingIndicator:boolean,floodZo...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM claims WHERE latitude = 29.8 LIMIT 2;\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-syndrome",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Select and Filter Blog](https://hendra-herviawan.github.io/sparksql-select-filter.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
