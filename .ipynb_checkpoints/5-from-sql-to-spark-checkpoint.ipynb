{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "frozen-weekend",
   "metadata": {},
   "source": [
    "# Exploring Data with Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-diary",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-relief",
   "metadata": {},
   "source": [
    "Now one of the main benefits of working with Pyspark is the ability to explore large datasets.  In this lesson, we'll work with Houston Flood Insurance claims data to do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20522293-1e74-4ad3-a68e-531406ec0f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/jeffreykatz/opt/anaconda3/lib/python3.9/site-packages (3.4.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/jeffreykatz/opt/anaconda3/lib/python3.9/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "every-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"Python Spark SQL basic example\") \\\n",
    "#     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-document",
   "metadata": {},
   "source": [
    "Then, let's read in the our csv data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "historic-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_df = spark.read.format(\"csv\").option(\"header\", \"true\").csv(\"./houston_claims.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-security",
   "metadata": {},
   "source": [
    "There are a lot of columns to this dataset, so let's set `vertical = True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "adjustable-shelf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------\n",
      " _c0                       | 0                    \n",
      " reportedCity              | HOUSTON              \n",
      " dateOfLoss                | 2017-08-27T00:00:... \n",
      " elevatedBuildingIndicator | False                \n",
      " floodZone                 | X                    \n",
      " latitude                  | 29.7                 \n",
      " longitude                 | -95.5                \n",
      " lowestFloodElevation      | null                 \n",
      " amountPaidOnBuildingClaim | 195857.43            \n",
      " amountPaidOnContentsClaim | 0.0                  \n",
      " yearofLoss                | 2017-01-01T00:00:... \n",
      " reportedZipcode           | 77096                \n",
      " id                        | 5e398d6774cbd479f... \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "claims_df.show(vertical = True, n = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-dress",
   "metadata": {},
   "source": [
    "And then let's take a look at the schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-powder",
   "metadata": {},
   "source": [
    "### Setting DataTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-highway",
   "metadata": {},
   "source": [
    "Now we can see that a number of columns are not in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dental-republican",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'),\n",
       " ('reportedCity', 'string'),\n",
       " ('dateOfLoss', 'string'),\n",
       " ('elevatedBuildingIndicator', 'string'),\n",
       " ('floodZone', 'string'),\n",
       " ('latitude', 'string'),\n",
       " ('longitude', 'string'),\n",
       " ('lowestFloodElevation', 'string'),\n",
       " ('amountPaidOnBuildingClaim', 'string'),\n",
       " ('amountPaidOnContentsClaim', 'string'),\n",
       " ('yearofLoss', 'string'),\n",
       " ('reportedZipcode', 'string'),\n",
       " ('id', 'string')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claims_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-september",
   "metadata": {},
   "source": [
    "For example, let's change `latitude` and `longitude` into floats, and the `amountPaid` columns into floats, and we can change the `yearOfLoss` column into an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "joint-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType, BooleanType, DateType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "familiar-mileage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "plain-sound",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_claims_df = claims_df.withColumn(\"yearOfLoss\", col(\"yearOfLoss\").cast(IntegerType())) \\\n",
    "    .withColumn(\"latitude\",col(\"latitude\").cast(FloatType())) \\\n",
    "    .withColumn(\"longitude\",col(\"longitude\").cast(FloatType())) \\\n",
    "    .withColumn(\"amountPaidOnBuildingClaim\",col(\"amountPaidOnBuildingClaim\").cast(IntegerType())) \\\n",
    "    .withColumn(\"amountPaidOnContentsClaim\",col(\"amountPaidOnContentsClaim\").cast(IntegerType())) \\\n",
    "    .withColumn(\"dateOfLoss\",col(\"dateOfLoss\").cast(DateType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-lottery",
   "metadata": {},
   "source": [
    "So we can see that we changed `latitude` and `longitude` into floats and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "pregnant-banana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'),\n",
       " ('reportedCity', 'string'),\n",
       " ('dateOfLoss', 'date'),\n",
       " ('elevatedBuildingIndicator', 'string'),\n",
       " ('floodZone', 'string'),\n",
       " ('latitude', 'float'),\n",
       " ('longitude', 'float'),\n",
       " ('lowestFloodElevation', 'string'),\n",
       " ('amountPaidOnBuildingClaim', 'int'),\n",
       " ('amountPaidOnContentsClaim', 'int'),\n",
       " ('yearOfLoss', 'int'),\n",
       " ('reportedZipcode', 'string'),\n",
       " ('id', 'string')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_claims_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "organizational-notion",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('houston_claims.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "economic-thanksgiving",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'int'),\n",
       " ('reportedCity', 'string'),\n",
       " ('dateOfLoss', 'timestamp'),\n",
       " ('elevatedBuildingIndicator', 'boolean'),\n",
       " ('floodZone', 'string'),\n",
       " ('latitude', 'double'),\n",
       " ('longitude', 'double'),\n",
       " ('lowestFloodElevation', 'double'),\n",
       " ('amountPaidOnBuildingClaim', 'double'),\n",
       " ('amountPaidOnContentsClaim', 'double'),\n",
       " ('yearofLoss', 'timestamp'),\n",
       " ('reportedZipcode', 'int'),\n",
       " ('id', 'string')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-congo",
   "metadata": {},
   "source": [
    "### Aggregate Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-bacon",
   "metadata": {},
   "source": [
    "Now we can get an overview of the data in each column with something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "parallel-perspective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+\n",
      "|summary|           latitude|          longitude|\n",
      "+-------+-------------------+-------------------+\n",
      "|  count|              19943|              19943|\n",
      "|   mean| 29.779967891591927| -95.44820225664996|\n",
      "| stddev|0.31282914829415176|0.48715072859743314|\n",
      "|    min|               29.5|             -149.8|\n",
      "|    max|               61.6|              -80.2|\n",
      "+-------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_claims_df.describe(['latitude', 'longitude']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-settle",
   "metadata": {},
   "source": [
    "Above, we use the `describe` method to calculate the `count`, `mean`, `stddev`, `min` and `max` of the `latitude` and `longitude` columns.  \n",
    "\n",
    "> If we did not pass through the list `['latitude', 'longitude']` in the `describe` function, spark would show us all of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "weekly-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, avg, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "happy-tackle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------+\n",
      "|      avg latitude|stddev_samp(latitude)|\n",
      "+------------------+---------------------+\n",
      "|29.779967891591927|  0.31282914829415176|\n",
      "+------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_claims_df.select([avg('latitude').alias(\"avg latitude\"),\n",
    "                          stddev('latitude')]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-circle",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Pyspark Operations](https://hendra-herviawan.github.io/)\n",
    "\n",
    "[Spark SQL string Functions](https://sparkbyexamples.com/spark/usage-of-spark-sql-string-functions/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-institution",
   "metadata": {},
   "source": [
    "[Pyspark From Pandas](https://databricks.com/session/data-wrangling-with-pyspark-for-data-scientists-who-know-pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-flashing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
